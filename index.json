[{"authors":["admin"],"categories":null,"content":"Hi, my name is Abhinav Jain. I work as a Research Engineer at IBM Research, India. I am broadly interested in multi-modal analytics where deep learning based algorithms are used to analyse content in text, images and videos for reasoning and further decision-making.\nI have worked for two years on IBM Watson Compare \u0026amp; Comply service for structured data extraction from business documents. I have also been working on smart data preparation for downstream processing in AI-based systems.\n","date":1568937600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1568937600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/abhinav-jain/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/abhinav-jain/","section":"authors","summary":"Hi, my name is Abhinav Jain. I work as a Research Engineer at IBM Research, India. I am broadly interested in multi-modal analytics where deep learning based algorithms are used to analyse content in text, images and videos for reasoning and further decision-making.","tags":null,"title":"Abhinav Jain","type":"authors"},{"authors":["Shashank Mujumdar","Nitin Gupta","Abhinav Jain","Douglas Burdick"],"categories":null,"content":"","date":1568937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568937600,"objectID":"05800c154678b72adf9be2be494247ef","permalink":"/publication/icdar-conf-paper/","publishdate":"2020-02-03T00:00:00Z","relpermalink":"/publication/icdar-conf-paper/","section":"publication","summary":"In this paper, we propose to combine the OCR performance into the loss function during training of single image super resolution (SISR) networks for document images.","tags":null,"title":"Simultaneous Optimisation of Image Quality Improvement and Text Content Extraction from Scanned Documents","type":"publication"},{"authors":["Nitin Gupta","Shashank Mujumdar","Prerna Agarwal","Abhinav Jain","Sameep Mehta"],"categories":null,"content":"","date":1557619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557619200,"objectID":"caa32ea1fb267bf8b7923d1b2e1ab847","permalink":"/publication/icaasp-conf-paper2/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/publication/icaasp-conf-paper2/","section":"publication","summary":"In this paper, we propose a novel way of training CNNs with a small subset of training samples using Deep Part Embeddings.","tags":null,"title":"Learning Convolutional Neural Networks with Deep Part Embeddings","type":"publication"},{"authors":["Abhinav Jain","Prerna Agarwal","Shashank Mujumdar","Nitin Gupta","Sameep Mehta","Chiranjoy Chattopadhyay"],"categories":null,"content":"","date":1557619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557619200,"objectID":"8cba6b38be86830d767d8300a3313fdf","permalink":"/publication/icaasp-conf-paper1/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/publication/icaasp-conf-paper1/","section":"publication","summary":"In this paper, we propose the Radial Loss which utilizes category and sub-category labels to learn an order-preserving fine-grained video similarity metric.","tags":null,"title":"Radial Loss for Learning Fine-grained Video Similarity Metric","type":"publication"},{"authors":["Nitin Gupta","Abhinav Jain","Prerna Agarwal","Shashank Mujumdar","Sameep Mehta"],"categories":null,"content":"","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"66139fd7ba1f3a9fe6e74f58eb46b5f4","permalink":"/publication/icpr-conf-paper/","publishdate":"2018-11-29T00:00:00Z","relpermalink":"/publication/icpr-conf-paper/","section":"publication","summary":"In this paper, we propose a novel pentuplet loss to learn the frame image similarity metric through a pentuplet-based deep learning framework.","tags":null,"title":"Pentuplet Loss for Simultaneous Shots and Critical Points Detection in a Video","type":"publication"},{"authors":["Abhinav Jain","Nitin Gupta","Shashank Mujumdar","Sameep Mehta","Rishi Madhok"],"categories":null,"content":"","date":1534118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534118400,"objectID":"74935625232aae8607476d0daf3f4031","permalink":"/publication/ht-conf-paper/","publishdate":"2018-07-03T00:00:00Z","relpermalink":"/publication/ht-conf-paper/","section":"publication","summary":"We propose a text enrichment framework that enrichest concepts form input text with their definitions, applications and a pre-requisite concept graph that showcases the inter-dependency within the extracted concepts.","tags":null,"title":"Content Driven Enrichment of Formal Text using Concept Definitions and Applications","type":"publication"},{"authors":null,"categories":null,"content":"Advertising in digital media often requires recognition of critical scenes in videos for smart placement of brand advertisements. These critical scenes raise viewer anxiety and are a part of some parent activity. We distinguish them from the rest of non-critical scenes using an order-preserving fine-grained similarity metric that learns the required representations. The learned metric is tested in two novel tasks: video critical scene recognition and fine-grained video retrieval. To learn the metric, we proposed Pentuplet Loss and recently, an improved and more robust Radial Loss.\nThese losses exploit the concept of `Quadlet Sampling\u0026rsquo; to mine data where each training sample is a tuple of query, positive, intermediate and negative samples. Finally, to ascertain the effectiveness of the loss in learning a deep metric for measuring similarities, we tested its performance against state-of-the-art baselines in the known tasks of fine-grained image retrieval and shot-boundary detection.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"bf17166ed022e0bc6939eecbcd97c74a","permalink":"/project/deep-metric-learning/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/deep-metric-learning/","section":"project","summary":"Video Representation Learning for Fine-Grained Scene Recognition and Retrieval.","tags":["Deep Learning","Event Recognition","Loss Formulation"],"title":"Deep Metric Learning","type":"project"},{"authors":null,"categories":null,"content":"In this project, we address the problem of re-training a deep neural network for a new class with limited training data (n to n+1 class learning) using a novel concept of Deep part embeddings (DPEs). DPEs are sub-networks of neuron activation extracted from a trained network identifying a visual and distinguishable element of a class. We identify visual elements that intuitively constitute a new class and extract the corresponding DPEs from the network pre-trained for the class sharing the identified visual element.\nFinally, we assemble them into a new network and re-train the model on limited samples of the new class and a subset of data from `n\u0026rsquo; classes to achieve high accuracy on the new class without significantly affecting the accuracy of n classes. We studied and generated results for DPE integration under two configurations- (i) sequential, when DPEs are sourced from different CNN architectures and (ii) shared; when DPEs are sourced from the same CNN architecture.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"46f5496d5b48500a5892a96f4e1e0a69","permalink":"/project/evolving-ai/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/evolving-ai/","section":"project","summary":"Model Learning with limited training data.","tags":["Deep Learning","Model Learning","Knowledge Transfer","Few Shot Learning"],"title":"Evolving AI","type":"project"},{"authors":["Shashank Mujumdar","Nitin Gupta","Abhinav Jain","Sameep Mehta"],"categories":null,"content":"","date":1512950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512950400,"objectID":"b150171433916a656d43a29c21117417","permalink":"/publication/ism-conf-paper/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/ism-conf-paper/","section":"publication","summary":"In this paper, we present a novel multistage framework to convert textual instructions into coherent visual descriptions (text instructions annotated with images).","tags":null,"title":"Coherent Visual Description of Textual Instructions","type":"publication"},{"authors":null,"categories":null,"content":"In practice, scanned PDF documents are converted into consumable representations (HTML/JSON) to drive structured data extraction. However, poor quality document images suffer from low token fidelity when an OCR engine such as Tessearct is used for token extraction. To remedy this, we leveraged deep learning based solutions for document quality enhancement and delivered the same for public release as part of IBM\u0026rsquo;s Watson API. We formulated a novel `Text Quality Improvement Loss\u0026rsquo; for the standard super-resolution convolutional neural network (SRCNN) to generate high-resolution text images. The proposed framework identifies text regions from images and minimizes additional MSE between such localised regions on top of the standard MSE, enforced by Single Image Super Resolution frameworks. This results in simultaneous optimisation of perceptual quality of the image and the OCR performance.\nMoreover, we provied (a) hybrid PDF support to extract data from documents containing both scanned and programmatic content, (b) capability to handle skewed documents, (c) multi-lingual support for data extraction from documents with over 50 different languages and (d) extraction service for detecting logos, bar-codes and signatures for downstream document processing such as querying, retrieval etc.\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"d1336e2581361fa74a1480c4fb93d75f","permalink":"/project/scanned-pdf-to-html/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/project/scanned-pdf-to-html/","section":"project","summary":"Extract structured information from unstructured documents.","tags":["OCR Improvement","Deep Learning","Super Resolution","Loss Formulation"],"title":"Scanned PDF-to-HTML Conversion","type":"project"},{"authors":null,"categories":null,"content":"Formal texts such as journal articles are composed of complex terminologies intended to be understood by targeted demographic. In absence of domain knowledge, they tend to be more ambiguous for general readers. To avail a complete semantic understanding of such texts for the readers, we proposed an enrichment system that mitigates the problem of searching for required information through heaps of sources. The system augments given text with required concept definitions, applications and concept dependency graphs.\nOur framework extracts key-concepts (technical terms) based on user discretion via a sequence of filtering stages - Linguistic Filtering, BBC Pruning and StackExchange Pruning. It detects the presence of required information by classifying each associated sentence into definition/application of the key-concept using a CNN-LSTM network. The same framework also runs on a data source such as Wikipedia to return the concept\u0026rsquo;s missing definition and real-life application.\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"c213a0a2c140bc9d65d0fb24f9a4f518","permalink":"/project/text-enrichment/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/project/text-enrichment/","section":"project","summary":"Enrichment of educational texts with supplementary information.","tags":["Deep Learning","AI for Education","Information Extraction"],"title":"Text Enrichment","type":"project"},{"authors":null,"categories":null,"content":"In this project, following multi-stage framework was developed to provide visual aid for a sequence of text based instructions in the form of coherent images associated with each of them - (a) For each instruction, visualisable phrases consisting of head action verbs and noun phrases are mined using standard practices like POS tagging, Dependency parsing and Co-reference resolution. (b) For each visualisable phrase, an API query is made to retrieve a set of images from a dataset crawled from sources such as WikiHow, Flickr, Google etc. Phrases and images together dictate the action being conducted in the instruction. (c) Across instructions sharing common information in the form of latent/non-latent entities, coherency is maintained using a graph based matching method utilising Dijkstra\u0026rsquo;s algorithm. A user study was conducted to validate improvement in understanding of text instructions and resemblance to actual ground truth.\n","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"e0af71e326cef183ca1ddbd03e1f4238","permalink":"/project/visual-description/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/project/visual-description/","section":"project","summary":"Provide visual aid for a sequence of text based instructions.","tags":["Deep Learning","Text-to-Image","Graph Matching"],"title":"Visual Cues for Text","type":"project"}]