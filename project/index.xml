<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Abhinav Jain</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Deep Metric Learning</title>
      <link>/project/deep-metric-learning/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/project/deep-metric-learning/</guid>
      <description>&lt;p&gt;Advertising in digital media often requires recognition of critical scenes in videos for smart placement of brand advertisements. These critical scenes raise viewer anxiety and are a part of some parent activity. We distinguish them from the rest of non-critical scenes using an order-preserving fine-grained similarity metric that learns the required representations. The learned metric is tested in two novel tasks: video critical scene recognition and fine-grained video retrieval. To learn the metric, we proposed Pentuplet Loss and recently, an improved and more robust Radial Loss.&lt;/p&gt;
&lt;p&gt;These losses exploit the concept of `Quadlet Sampling&amp;rsquo; to mine data where each training sample is a tuple of query, positive, intermediate and negative samples. Finally, to ascertain the effectiveness of the loss in learning a deep metric for measuring similarities, we tested its performance against state-of-the-art baselines in the known tasks of fine-grained image retrieval and shot-boundary detection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolving AI</title>
      <link>/project/evolving-ai/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/project/evolving-ai/</guid>
      <description>&lt;p&gt;In this project, we address the problem of re-training a deep neural network for a new class with limited training data (n to n+1 class learning) using a novel concept of Deep part embeddings (DPEs). DPEs are sub-networks of neuron activation extracted from a trained network identifying a visual and distinguishable element of a class. We identify visual elements that intuitively constitute a new class and extract the corresponding DPEs from the network pre-trained for the class sharing the identified visual element.&lt;/p&gt;
&lt;p&gt;Finally, we assemble them into a new network and re-train the model on limited samples of the new class and a subset of data from `n&amp;rsquo; classes to achieve high accuracy on the new class without significantly affecting the accuracy of n classes. We studied and generated results for DPE integration under two configurations- (i) sequential, when DPEs are sourced from different CNN architectures and (ii) shared; when DPEs are sourced from the same CNN architecture.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scanned PDF-to-HTML Conversion</title>
      <link>/project/scanned-pdf-to-html/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/project/scanned-pdf-to-html/</guid>
      <description>&lt;p&gt;In practice, scanned PDF documents are converted into consumable representations (HTML/JSON) to drive structured data extraction. However, poor quality document images suffer from low token fidelity when an OCR engine such as Tessearct is used for token extraction.  To remedy this, we leveraged deep learning based solutions for document quality enhancement and delivered the same for public release as part of IBM&amp;rsquo;s Watson API. We formulated a novel `Text Quality Improvement Loss&amp;rsquo; for the standard super-resolution convolutional neural network (SRCNN) to generate high-resolution text images. The proposed framework identifies text regions from images and minimizes additional MSE between such localised regions on top of the standard MSE, enforced by Single Image Super Resolution frameworks. This results in simultaneous optimisation of perceptual quality of the image and the OCR performance.&lt;/p&gt;
&lt;p&gt;Moreover, we provied (a) hybrid PDF support to extract data from documents containing both scanned and programmatic content, (b) capability to handle skewed documents, (c) multi-lingual support for data extraction from documents with over 50 different languages and (d) extraction service for detecting logos, bar-codes and signatures for downstream document processing such as querying, retrieval etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text Enrichment</title>
      <link>/project/text-enrichment/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/project/text-enrichment/</guid>
      <description>&lt;p&gt;Formal texts such as journal articles are composed of complex terminologies intended to be understood by targeted demographic. In absence of domain knowledge, they tend to be more ambiguous for general readers. To avail a complete semantic understanding of such texts for the readers, we proposed an enrichment system that mitigates the problem of searching for required information through heaps of sources. The system augments given text with required concept definitions, applications and concept dependency graphs.&lt;/p&gt;
&lt;p&gt;Our framework extracts key-concepts (technical terms) based on user discretion via a sequence of filtering stages - Linguistic Filtering, BBC Pruning and StackExchange Pruning. It detects the presence of required information by classifying each associated sentence into definition/application of the key-concept using a CNN-LSTM network. The same framework also runs on a data source such as Wikipedia to return the concept&amp;rsquo;s missing definition and real-life application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Cues for Text</title>
      <link>/project/visual-description/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 +0000</pubDate>
      <guid>/project/visual-description/</guid>
      <description>&lt;p&gt;In this project, following multi-stage framework was developed to provide visual aid for a sequence of text based instructions in the form of coherent images associated with each of them - (a) For each instruction, visualisable phrases consisting of head action verbs and noun phrases are mined using standard practices like POS tagging, Dependency parsing and Co-reference resolution. (b) For each visualisable phrase, an API query is made to retrieve a set of images from a dataset crawled from sources such as WikiHow, Flickr, Google etc. Phrases and images together dictate the action being conducted in the instruction. (c) Across instructions sharing common information in the form of latent/non-latent entities, coherency is maintained using a graph based matching method utilising Dijkstra&amp;rsquo;s algorithm. A user study was conducted to validate improvement in understanding of text instructions and resemblance to actual ground truth.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
